{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsC2NW_M0kwL",
        "outputId": "734fd8e8-b13a-4480-cf66-9c6a80f01f7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboard_logger in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (3.17.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (7.1.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboard_logger) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-igraph in /usr/local/lib/python3.7/dist-packages (0.9.11)\n",
            "Requirement already satisfied: igraph==0.9.11 in /usr/local/lib/python3.7/dist-packages (from python-igraph) (0.9.11)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from igraph==0.9.11->python-igraph) (1.6.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install tensorboard_logger\n",
        "!pip install python-igraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PygCEVtu6mXX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R3QAh1Q86n-p"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kqsqFx7n6rnY"
      },
      "outputs": [],
      "source": [
        "# choose a local (colab) directory to store the data.\n",
        "local_download_path = os.path.expanduser('~/data')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e3mJlwrv671q"
      },
      "outputs": [],
      "source": [
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1iYYZM9TK0J2EANl6p49X5cvq2P2mRIdK' in parents\"}).GetList()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukTOf4Cx7cpU",
        "outputId": "6bc822e5-105d-4df8-d7c6-fbb30c812280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "title: vertex_id.npy, id: 1Ebb6VhK9M7d-IC9lrD6UvPerJqQX4kqP\n",
            "downloading to /root/data/vertex_id.npy\n",
            "title: adjacency_matrix.npy, id: 1ZdTKHsddzV7WQ20Mrjaze4M87xYhfAu2\n",
            "downloading to /root/data/adjacency_matrix.npy\n",
            "title: label.npy, id: 1cl1KyCRphRfVl34tAmX_GhW7FfYBhldt\n",
            "downloading to /root/data/label.npy\n",
            "title: influence_feature.npy, id: 1pv9nt-Lk-wyGTuRSDNTupVaEtUVsfBW6\n",
            "downloading to /root/data/influence_feature.npy\n",
            "title: deepwalk.emb_64, id: 14lcv0BelrMeM5WjK-LCr0rn2LQ7BFTqq\n",
            "downloading to /root/data/deepwalk.emb_64\n",
            "title: vertex_feature.npy, id: 1I1YSIC8fKPMHUSueBHVkfqADvixHMDt_\n",
            "downloading to /root/data/vertex_feature.npy\n"
          ]
        }
      ],
      "source": [
        "for f in file_list:\n",
        "  # 3. Create & download by id.\n",
        "  print('title: %s, id: %s' % (f['title'], f['id']))\n",
        "  fname = os.path.join(local_download_path, f['title'])\n",
        "  print('downloading to {}'.format(fname))\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M8igOpcy9Z18"
      },
      "outputs": [],
      "source": [
        "labels_path = os.path.join(local_download_path, 'label.npy')\n",
        "adjacency_matrix_path = os.path.join(local_download_path, 'adjacency_matrix.npy')\n",
        "deepwalk_path = os.path.join(local_download_path, 'deepwalk.emb_64')\n",
        "influence_feature_path = os.path.join(local_download_path, 'influence_feature.npy')\n",
        "vertex_feature_path = os.path.join(local_download_path, 'vertex_feature.npy')\n",
        "vertex_id_path = os.path.join(local_download_path, 'vertex_id.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AHVD2GAZ_JFf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import Sampler\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import sklearn\n",
        "import itertools\n",
        "import logging\n",
        "import igraph\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import time\n",
        "import argparse\n",
        "import shutil\n",
        "import logging\n",
        "logging.basicConfig(level = logging.INFO)\n",
        "from tensorboard_logger import tensorboard_logger\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Ms6S-GoAA0E"
      },
      "outputs": [],
      "source": [
        "def load_w2v_feature(file, max_idx=0):\n",
        "    with open(file, \"rb\") as f:\n",
        "        nu = 0\n",
        "        for line in f:\n",
        "            content = line.strip().split()\n",
        "            nu += 1\n",
        "            if nu == 1:\n",
        "                n, d = int(content[0]), int(content[1])\n",
        "                feature = [[0.] * d for i in range(max(n, max_idx + 1))]\n",
        "                continue\n",
        "            index = int(content[0])\n",
        "            while len(feature) <= index:\n",
        "                feature.append([0.] * d)\n",
        "            for i, x in enumerate(content[1:]):\n",
        "                feature[index][i] = float(x)\n",
        "    for item in feature:\n",
        "        assert len(item) == d\n",
        "    return np.array(feature, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LK4pxDhVAeh5"
      },
      "outputs": [],
      "source": [
        "class ChunkSampler(Sampler):\n",
        "    \"\"\"\n",
        "    Samples elements sequentially from some offset.\n",
        "    Arguments:\n",
        "        num_samples: # of desired datapoints\n",
        "        start: offset where we should start selecting from\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples, start=0):\n",
        "        self.num_samples = num_samples\n",
        "        self.start = start\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(range(self.start, self.start + self.num_samples))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "\n",
        "class InfluenceDataSet(Dataset):\n",
        "    def __init__(self, file_dir, embedding_dim, seed, shuffle, model):\n",
        "        self.graphs = np.load(os.path.join(file_dir, \"adjacency_matrix.npy\")).astype(np.float32)\n",
        "\n",
        "        # self-loop trick, the input graphs should have no self-loop\n",
        "        identity = np.identity(self.graphs.shape[1])\n",
        "        self.graphs += identity\n",
        "        self.graphs[self.graphs != 0] = 1.0\n",
        "\n",
        "        # normalized graph laplacian for GCN: D^{-1/2}AD^{-1/2}\n",
        "        for i in range(len(self.graphs)):\n",
        "            graph = self.graphs[i]\n",
        "            d_root_inv = 1. / np.sqrt(np.sum(graph, axis=1))\n",
        "            graph = (graph.T * d_root_inv).T * d_root_inv\n",
        "\n",
        "        logger.info(\"graphs loaded!\")\n",
        "\n",
        "        # wheather a user has been influenced\n",
        "        # wheather he/she is the ego user\n",
        "        self.influence_features = np.load(\n",
        "                os.path.join(file_dir, \"influence_feature.npy\")).astype(np.float32)\n",
        "        logger.info(\"influence features loaded!\")\n",
        "\n",
        "        self.labels = np.load(os.path.join(file_dir, \"label.npy\"))\n",
        "        logger.info(\"labels loaded!\")\n",
        "\n",
        "        self.vertices = np.load(os.path.join(file_dir, \"vertex_id.npy\"))\n",
        "        logger.info(\"vertex ids loaded!\")\n",
        "\n",
        "        if shuffle:\n",
        "            self.graphs, self.influence_features, self.labels, self.vertices = \\\n",
        "                    sklearn.utils.shuffle(\n",
        "                        self.graphs, self.influence_features,\n",
        "                        self.labels, self.vertices,\n",
        "                        random_state=seed\n",
        "                    )\n",
        "\n",
        "        vertex_features = np.load(os.path.join(file_dir, \"vertex_feature.npy\"))\n",
        "        vertex_features = preprocessing.scale(vertex_features)\n",
        "        self.vertex_features = torch.FloatTensor(vertex_features)\n",
        "        logger.info(\"global vertex features loaded!\")\n",
        "\n",
        "        embedding_path = os.path.join(file_dir, \"deepwalk.emb_%d\" % embedding_dim)\n",
        "        max_vertex_idx = np.max(self.vertices)\n",
        "        embedding = load_w2v_feature(embedding_path, max_vertex_idx)\n",
        "        self.embedding = torch.FloatTensor(embedding)\n",
        "        logger.info(\"%d-dim embedding loaded!\", embedding_dim)\n",
        "\n",
        "        self.N = self.graphs.shape[0]\n",
        "        logger.info(\"%d ego networks loaded, each with size %d\" % (self.N, self.graphs.shape[1]))\n",
        "\n",
        "        n_classes = self.get_num_class()\n",
        "        class_weight = self.N / (n_classes * np.bincount(self.labels))\n",
        "        self.class_weight = torch.FloatTensor(class_weight)\n",
        "\n",
        "    def get_embedding(self):\n",
        "        return self.embedding\n",
        "\n",
        "    def get_influence_features(self):\n",
        "        return self.influence_features\n",
        "\n",
        "    def get_vertex_features(self):\n",
        "        return self.vertex_features\n",
        "\n",
        "    def get_feature_dimension(self):\n",
        "        return self.influence_features.shape[-1]\n",
        "\n",
        "    def get_num_class(self):\n",
        "        return np.unique(self.labels).shape[0]\n",
        "\n",
        "    def get_class_weight(self):\n",
        "        return self.class_weight\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.N\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.graphs[idx], self.influence_features[idx], self.labels[idx], self.vertices[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JU7HHkc_PM9l"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger('__main__.' + __name__)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s') # include timestamp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2vbcTILVHE2G"
      },
      "outputs": [],
      "source": [
        "# Training settings\n",
        "class args:\n",
        "  tensorboard_log=''              #name of this run\n",
        "  model = 'gcn'                   #models used\n",
        "  no_cuda=False                   #Disables CUDA training.\n",
        "  seed=42                         #Random seed.\n",
        "  epochs=100                     #Number of epochs to train.\n",
        "  lr=0.05                         #Initial learning rate.\n",
        "  weight_decay=1e-3               #Weight decay (L2 loss on parameters).\n",
        "  dropout=0.005                     #Dropout rate (1 - keep probability).\n",
        "  hidden_units=\"16,8\"             #Hidden units in each hidden layer, splitted with comma\n",
        "  heads=\"1,1,1\"                   #Heads in each layer, splitted with comma\n",
        "  batch=128                      #Batch size\n",
        "  dim=64                          #Embedding dimension\n",
        "  check_point=10                  #Eheck point\n",
        "  instance_normalization=True    #Enable instance normalization\n",
        "  shuffle=False                   #Shuffle dataset\n",
        "  file_dir=local_download_path    #Input file directory\n",
        "  train_ratio=50                  #Training ratio (0, 100)\n",
        "  valid_ratio=25.5                  #Validation ratio (0, 100)\n",
        "  class_weight_balanced=False     #Adjust weights inversely proportional to class frequencies in the input data\n",
        "  use_vertex_feature=True         #Whether to use vertices' structural features\n",
        "  sequence_size=16                #Sequence size (only useful for pscn)\n",
        "  neighbor_size=5                 #Neighborhood size (only useful for pscn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z_gfpfELLJ8s"
      },
      "outputs": [],
      "source": [
        "args.cuda = not args.no_cuda and torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kCQWIEXgOe0x"
      },
      "outputs": [],
      "source": [
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JSNy6Kl8O6DP"
      },
      "outputs": [],
      "source": [
        "# adj N*n*n\n",
        "# feature N*n*f\n",
        "# labels N*n*c\n",
        "# Load data\n",
        "# vertex: vertex id in global network N*n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b986Tif8Qa8t"
      },
      "outputs": [],
      "source": [
        "influence_dataset = InfluenceDataSet(\n",
        "            args.file_dir, args.dim, args.seed, args.shuffle, args.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pGPkIH-mRBzZ"
      },
      "outputs": [],
      "source": [
        "N = len(influence_dataset)\n",
        "n_classes = 2\n",
        "class_weight = influence_dataset.get_class_weight() \\\n",
        "        if args.class_weight_balanced else torch.ones(n_classes)\n",
        "logger.info(\"class_weight=%.2f:%.2f\", class_weight[0], class_weight[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2OZ3yW9HSx71"
      },
      "outputs": [],
      "source": [
        "feature_dim = influence_dataset.get_feature_dimension()\n",
        "n_units = [feature_dim] + [int(x) for x in args.hidden_units.strip().split(\",\")] + [n_classes]\n",
        "logger.info(\"feature dimension=%d\", feature_dim)\n",
        "logger.info(\"number of classes=%d\", n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D8gckmx0iSNd"
      },
      "outputs": [],
      "source": [
        "train_start,  valid_start, test_start = \\\n",
        "        0, int(N * args.train_ratio / 100), int(N * (args.train_ratio + args.valid_ratio) / 100)\n",
        "train_loader = DataLoader(influence_dataset, batch_size=args.batch,\n",
        "                        sampler=ChunkSampler(valid_start - train_start, 0))\n",
        "valid_loader = DataLoader(influence_dataset, batch_size=args.batch,\n",
        "                        sampler=ChunkSampler(test_start - valid_start, valid_start))\n",
        "test_loader = DataLoader(influence_dataset, batch_size=args.batch,\n",
        "                        sampler=ChunkSampler(N - test_start, test_start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W-iDMv6sqtkX"
      },
      "outputs": [],
      "source": [
        "class BatchGraphConvolution(Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(BatchGraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.Tensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_features))\n",
        "            init.constant_(self.bias, 0)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, x, lap):\n",
        "        expand_weight = self.weight.expand(x.shape[0], -1, -1)\n",
        "        support = torch.bmm(x, expand_weight)\n",
        "        output = torch.bmm(lap, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a7YADToFqmFG"
      },
      "outputs": [],
      "source": [
        "class BatchGCN(nn.Module):\n",
        "    def __init__(self, n_units, dropout, pretrained_emb, vertex_feature,\n",
        "            use_vertex_feature, fine_tune=False, instance_normalization=False):\n",
        "        super(BatchGCN, self).__init__()\n",
        "        self.num_layer = len(n_units) - 1\n",
        "        self.dropout = dropout\n",
        "        self.inst_norm = instance_normalization\n",
        "        if self.inst_norm:\n",
        "            self.norm = nn.InstanceNorm1d(pretrained_emb.size(1), momentum=0.0, affine=True)\n",
        "\n",
        "        # https://discuss.pytorch.org/t/can-we-use-pre-trained-word-embeddings-for-weight-initialization-in-nn-embedding/1222/2\n",
        "        self.embedding = nn.Embedding(pretrained_emb.size(0), pretrained_emb.size(1))\n",
        "        self.embedding.weight = nn.Parameter(pretrained_emb)\n",
        "        self.embedding.weight.requires_grad = fine_tune\n",
        "        n_units[0] += pretrained_emb.size(1)\n",
        "\n",
        "        self.use_vertex_feature = use_vertex_feature\n",
        "        if self.use_vertex_feature:\n",
        "            self.vertex_feature = nn.Embedding(vertex_feature.size(0), vertex_feature.size(1))\n",
        "            self.vertex_feature.weight = nn.Parameter(vertex_feature)\n",
        "            self.vertex_feature.weight.requires_grad = False\n",
        "            n_units[0] += vertex_feature.size(1)\n",
        "\n",
        "        self.layer_stack = nn.ModuleList()\n",
        "\n",
        "        for i in range(self.num_layer):\n",
        "            self.layer_stack.append(\n",
        "                    BatchGraphConvolution(n_units[i], n_units[i + 1])\n",
        "                    )\n",
        "\n",
        "    def forward(self, x, vertices, lap):\n",
        "        emb = self.embedding(vertices)\n",
        "        if self.inst_norm:\n",
        "            emb = self.norm(emb.transpose(1, 2)).transpose(1, 2)\n",
        "        x = torch.cat((x, emb), dim=2)\n",
        "        if self.use_vertex_feature:\n",
        "            vfeature = self.vertex_feature(vertices)\n",
        "            x = torch.cat((x, vfeature), dim=2)\n",
        "        for i, gcn_layer in enumerate(self.layer_stack):\n",
        "            x = gcn_layer(x, lap)\n",
        "            if i + 1 < self.num_layer:\n",
        "                x = F.elu(x)\n",
        "                x = F.dropout(x, self.dropout, training=self.training)\n",
        "        return F.log_softmax(x, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "teNCj-6zjqCq"
      },
      "outputs": [],
      "source": [
        "class MultiHeadGraphAttention(nn.Module):\n",
        "    def __init__(self, n_head, f_in, f_out, attn_dropout, bias=True):\n",
        "        super(MultiHeadGraphAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.w = Parameter(torch.Tensor(n_head, f_in, f_out))\n",
        "        self.a_src = Parameter(torch.Tensor(n_head, f_out, 1))\n",
        "        self.a_dst = Parameter(torch.Tensor(n_head, f_out, 1))\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(f_out))\n",
        "            init.constant_(self.bias, 0)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        init.xavier_uniform_(self.w)\n",
        "        init.xavier_uniform_(self.a_src)\n",
        "        init.xavier_uniform_(self.a_dst)\n",
        "\n",
        "    def forward(self, h, adj):\n",
        "        n = h.size(0) # h is of size n x f_in\n",
        "        h_prime = torch.matmul(h.unsqueeze(0), self.w) #  n_head x n x f_out\n",
        "        attn_src = torch.bmm(h_prime, self.a_src) # n_head x n x 1\n",
        "        attn_dst = torch.bmm(h_prime, self.a_dst) # n_head x n x 1\n",
        "        attn = attn_src.expand(-1, -1, n) + attn_dst.expand(-1, -1, n).permute(0, 2, 1) # n_head x n x n\n",
        "\n",
        "        attn = self.leaky_relu(attn)\n",
        "        attn.data.masked_fill_(1 - adj, float(\"-inf\"))\n",
        "        attn = self.softmax(attn) # n_head x n x n\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.bmm(attn, h_prime) # n_head x n x f_out\n",
        "\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "class BatchMultiHeadGraphAttention(nn.Module):\n",
        "    def __init__(self, n_head, f_in, f_out, attn_dropout, bias=True):\n",
        "        super(BatchMultiHeadGraphAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.w = Parameter(torch.Tensor(n_head, f_in, f_out))\n",
        "        self.a_src = Parameter(torch.Tensor(n_head, f_out, 1))\n",
        "        self.a_dst = Parameter(torch.Tensor(n_head, f_out, 1))\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(f_out))\n",
        "            init.constant_(self.bias, 0)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        init.xavier_uniform_(self.w)\n",
        "        init.xavier_uniform_(self.a_src)\n",
        "        init.xavier_uniform_(self.a_dst)\n",
        "\n",
        "    def forward(self, h, adj):\n",
        "        bs, n = h.size()[:2] # h is of size bs x n x f_in\n",
        "        h_prime = torch.matmul(h.unsqueeze(1), self.w) # bs x n_head x n x f_out\n",
        "        attn_src = torch.matmul(torch.tanh(h_prime), self.a_src) # bs x n_head x n x 1\n",
        "        attn_dst = torch.matmul(torch.tanh(h_prime), self.a_dst) # bs x n_head x n x 1\n",
        "        attn = attn_src.expand(-1, -1, -1, n) + attn_dst.expand(-1, -1, -1, n).permute(0, 1, 3, 2) # bs x n_head x n x n\n",
        "\n",
        "        attn = self.leaky_relu(attn)\n",
        "        mask = 1 - adj # bs x 1 x n x n\n",
        "        attn.data.masked_fill_(mask, float(\"-inf\"))\n",
        "        attn = self.softmax(attn) # bs x n_head x n x n\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.matmul(attn, h_prime) # bs x n_head x n x f_out\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pd4XV8IdjzUK"
      },
      "outputs": [],
      "source": [
        "class BatchGAT(nn.Module):\n",
        "    def __init__(self, pretrained_emb, vertex_feature, use_vertex_feature,\n",
        "            n_units=[1433, 8, 7], n_heads=[4, 1],\n",
        "            dropout=0.1, attn_dropout=0.0, fine_tune=False,\n",
        "            instance_normalization=False):\n",
        "        super(BatchGAT, self).__init__()\n",
        "        self.n_layer = len(n_units) - 1\n",
        "        self.dropout = dropout\n",
        "        self.inst_norm = instance_normalization\n",
        "        if self.inst_norm:\n",
        "            self.norm = nn.InstanceNorm1d(pretrained_emb.size(1), momentum=0.0, affine=True)\n",
        "\n",
        "        # https://discuss.pytorch.org/t/can-we-use-pre-trained-word-embeddings-for-weight-initialization-in-nn-embedding/1222/2\n",
        "        self.embedding = nn.Embedding(pretrained_emb.size(0), pretrained_emb.size(1))\n",
        "        self.embedding.weight = nn.Parameter(pretrained_emb)\n",
        "        self.embedding.weight.requires_grad = fine_tune\n",
        "        n_units[0] += pretrained_emb.size(1)\n",
        "\n",
        "        self.use_vertex_feature = use_vertex_feature\n",
        "        if self.use_vertex_feature:\n",
        "            self.vertex_feature = nn.Embedding(vertex_feature.size(0), vertex_feature.size(1))\n",
        "            self.vertex_feature.weight = nn.Parameter(vertex_feature)\n",
        "            self.vertex_feature.weight.requires_grad = False\n",
        "            n_units[0] += vertex_feature.size(1)\n",
        "\n",
        "        self.layer_stack = nn.ModuleList()\n",
        "        for i in range(self.n_layer):\n",
        "            # consider multi head from last layer\n",
        "            f_in = n_units[i] * n_heads[i - 1] if i else n_units[i]\n",
        "            self.layer_stack.append(\n",
        "                    BatchMultiHeadGraphAttention(n_heads[i], f_in=f_in,\n",
        "                        f_out=n_units[i + 1], attn_dropout=attn_dropout)\n",
        "                    )\n",
        "\n",
        "    def forward(self, x, vertices, adj):\n",
        "        emb = self.embedding(vertices)\n",
        "        if self.inst_norm:\n",
        "            emb = self.norm(emb.transpose(1, 2)).transpose(1, 2)\n",
        "        x = torch.cat((x, emb), dim=2)\n",
        "        if self.use_vertex_feature:\n",
        "            vfeature = self.vertex_feature(vertices)\n",
        "            x = torch.cat((x, vfeature), dim=2)\n",
        "        bs, n = adj.size()[:2]\n",
        "        for i, gat_layer in enumerate(self.layer_stack):\n",
        "            x = gat_layer(x, adj) # bs x n_head x n x f_out\n",
        "            if i + 1 == self.n_layer:\n",
        "                x = x.mean(dim=1)\n",
        "            else:\n",
        "                x = F.elu(x.transpose(1, 2).contiguous().view(bs, n, -1))\n",
        "                x = F.dropout(x, self.dropout, training=self.training)\n",
        "        return F.log_softmax(x, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZRobHX1FkBnI"
      },
      "outputs": [],
      "source": [
        "model = BatchGCN(pretrained_emb=influence_dataset.get_embedding(),\n",
        "            vertex_feature=influence_dataset.get_vertex_features(),\n",
        "            use_vertex_feature=args.use_vertex_feature,\n",
        "            n_units=n_units,\n",
        "            dropout=args.dropout,\n",
        "            instance_normalization=args.instance_normalization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vzzIhzakkUV2"
      },
      "outputs": [],
      "source": [
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    class_weight = class_weight.cuda()\n",
        "\n",
        "params = [{'params': filter(lambda p: p.requires_grad, model.parameters())\n",
        "    if args.model == \"pscn\" else model.layer_stack.parameters()}]\n",
        "\n",
        "optimizer = optim.Adagrad(params, lr=args.lr, weight_decay=args.weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EEa6EgGQkicu"
      },
      "outputs": [],
      "source": [
        "def evaluate(epoch, loader, thr=None, return_best_thr=False, log_desc='valid_'):\n",
        "    model.eval()\n",
        "    total = 0.\n",
        "    loss, prec, rec, f1 = 0., 0., 0., 0.\n",
        "    y_true, y_pred, y_score = [], [], []\n",
        "    for i_batch, batch in enumerate(loader):\n",
        "        graph, features, labels, vertices = batch\n",
        "        bs = graph.size(0)\n",
        "\n",
        "        if args.cuda:\n",
        "            features = features.cuda()\n",
        "            graph = graph.cuda()\n",
        "            labels = labels.cuda()\n",
        "            vertices = vertices.cuda()\n",
        "\n",
        "        output = model(features, vertices, graph)\n",
        "        if args.model == \"gcn\" or args.model == \"gat\":\n",
        "            output = output[:, -1, :]\n",
        "        loss_batch = F.nll_loss(output, labels, class_weight)\n",
        "        loss += bs * loss_batch.item()\n",
        "\n",
        "        y_true += labels.data.tolist()\n",
        "        y_pred += output.max(1)[1].data.tolist()\n",
        "        y_score += output[:, 1].data.tolist()\n",
        "        total += bs\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    if thr is not None:\n",
        "        logger.info(\"using threshold %.4f\", thr)\n",
        "        y_score = np.array(y_score)\n",
        "        y_pred = np.zeros_like(y_score)\n",
        "        y_pred[y_score > thr] = 1\n",
        "\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
        "    auc = roc_auc_score(y_true, y_score)\n",
        "    logger.info(\"%sloss: %.4f AUC: %.4f Prec: %.4f Rec: %.4f F1: %.4f\",\n",
        "            log_desc, loss / total, auc, prec, rec, f1)\n",
        "\n",
        "    tensorboard_logger.Logger(log_desc + 'loss', loss / total, epoch + 1)\n",
        "    tensorboard_logger.Logger(log_desc + 'auc', auc, epoch + 1)\n",
        "    tensorboard_logger.Logger(log_desc + 'prec', prec, epoch + 1)\n",
        "    tensorboard_logger.Logger(log_desc + 'rec', rec, epoch + 1)\n",
        "    tensorboard_logger.Logger(log_desc + 'f1', f1, epoch + 1)\n",
        "\n",
        "    if return_best_thr:\n",
        "        precs, recs, thrs = precision_recall_curve(y_true, y_score)\n",
        "        f1s = 2 * precs * recs / (precs + recs)\n",
        "        f1s = f1s[:-1]\n",
        "        thrs = thrs[~np.isnan(f1s)]\n",
        "        f1s = f1s[~np.isnan(f1s)]\n",
        "        best_thr = thrs[np.argmax(f1s)]\n",
        "        logger.info(\"best threshold=%4f, f1=%.4f\", best_thr, np.max(f1s))\n",
        "        return best_thr\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FocD7AgWkm_G"
      },
      "outputs": [],
      "source": [
        "def train(epoch, train_loader, valid_loader, test_loader, log_desc='train_'):\n",
        "    model.train()\n",
        "\n",
        "    loss = 0.\n",
        "    total = 0.\n",
        "    for i_batch, batch in enumerate(train_loader):\n",
        "        graph, features, labels, vertices = batch\n",
        "        bs = graph.size(0)\n",
        "\n",
        "        if args.cuda:\n",
        "            features = features.cuda()\n",
        "            graph = graph.cuda()\n",
        "            labels = labels.cuda()\n",
        "            vertices = vertices.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(features, vertices, graph)\n",
        "        if args.model == \"gcn\" or args.model == \"gat\":\n",
        "            output = output[:, -1, :]\n",
        "        loss_train = F.nll_loss(output, labels, class_weight)\n",
        "        loss += bs * loss_train.item()\n",
        "        total += bs\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "    logger.info(\"train loss in this epoch %f\", loss / total)\n",
        "    tensorboard_logger.Logger('train_loss', loss / total, epoch + 1)\n",
        "    if (epoch + 1) % args.check_point == 0:\n",
        "        logger.info(\"epoch %d, checkpoint!\", epoch)\n",
        "        best_thr = evaluate(epoch, valid_loader, return_best_thr=True, log_desc='valid_')\n",
        "        evaluate(epoch, test_loader, thr=best_thr, log_desc='test_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3hIXW9Okst2",
        "outputId": "5cbe7409-2443-4909-b6ac-848b823d4188"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "\n",
        "t_total = time.time()\n",
        "logger.info(\"training...\")\n",
        "for epoch in range(args.epochs):\n",
        "    train(epoch, train_loader, valid_loader, test_loader)\n",
        "logger.info(\"optimization Finished!\")\n",
        "logger.info(\"total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "logger.info(\"retrieve best threshold...\")\n",
        "best_thr = evaluate(args.epochs, valid_loader, return_best_thr=True, log_desc='valid_')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}